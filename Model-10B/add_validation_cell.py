#!/usr/bin/env python3
"""
Add final validation test cell to misc.ipynb
Tests model on 10 diverse locations with expert predictions vs model predictions
"""

import json

notebook_path = 'misc.ipynb'

# The validation cell code
validation_cell_code = [
    "# ==================== FINAL MODEL VALIDATION TEST ====================\n",
    "# Testing calibrated model (v2) on 10 diverse locations\n",
    "# Comparing expert predictions vs model predictions\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üî¨ FINAL MODEL VALIDATION TEST\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Testing model: phase2_epoch_5_calibrated_v2.pth (+8.0 calibration)\")\n",
    "print(f\"Comparing expert predictions vs model predictions\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Test locations with expert predictions\n",
    "# Format: (lat, lon, name, dates, expected_degradation, expected_range, degradation_type)\n",
    "validation_locations = [\n",
    "    # === HIGH DEGRADATION (Expected: 60-90%) ===\n",
    "    (40.15, -120.85, \"Dixie Fire Burn Scar\", ('2022-06-01', '2022-09-01'), \n",
    "     \"High (70-85%)\", (0.70, 0.85), \"Wildfire - severe burn\"),\n",
    "    \n",
    "    (-9.50, -63.60, \"Amazon Deforestation\", ('2024-06-01', '2024-08-31'), \n",
    "     \"High (65-80%)\", (0.65, 0.80), \"Deforestation - clear cutting\"),\n",
    "    \n",
    "    (29.35, -90.10, \"Barataria Bay Erosion\", ('2024-03-01', '2024-05-01'), \n",
    "     \"High (60-75%)\", (0.60, 0.75), \"Coastal erosion - land loss\"),\n",
    "    \n",
    "    # === MEDIUM DEGRADATION (Expected: 30-60%) ===\n",
    "    (40.15, -120.85, \"Dixie Fire Recovery\", ('2024-06-01', '2024-09-01'), \n",
    "     \"Medium (35-55%)\", (0.35, 0.55), \"Post-fire recovery 2 years\"),\n",
    "    \n",
    "    (29.30, -89.25, \"Saltwater Intrusion\", ('2024-03-01', '2024-05-01'), \n",
    "     \"Medium (40-60%)\", (0.40, 0.60), \"Saltwater intrusion - vegetation stress\"),\n",
    "    \n",
    "    (29.80, -93.00, \"Hurricane Impact\", ('2023-06-01', '2023-08-31'), \n",
    "     \"Medium (45-65%)\", (0.45, 0.65), \"Hurricane damage - partial recovery\"),\n",
    "    \n",
    "    # === LOW DEGRADATION (Expected: 0-30%) ===\n",
    "    (40.00, -121.30, \"Plumas Healthy Forest\", ('2024-06-01', '2024-09-01'), \n",
    "     \"Low (5-15%)\", (0.05, 0.15), \"Healthy forest - no disturbance\"),\n",
    "    \n",
    "    (29.75, -90.10, \"Jean Lafitte Protected\", ('2024-03-01', '2024-05-01'), \n",
    "     \"Low (10-20%)\", (0.10, 0.20), \"Protected wetland - healthy\"),\n",
    "    \n",
    "    (30.25, -91.50, \"Atchafalaya Healthy\", ('2024-03-01', '2024-05-01'), \n",
    "     \"Low (5-15%)\", (0.05, 0.15), \"Healthy swamp - no degradation\"),\n",
    "    \n",
    "    # === EDGE CASE ===\n",
    "    (29.20, -90.75, \"Terrebonne Severe Loss\", ('2024-03-01', '2024-05-01'), \n",
    "     \"Very High (75-90%)\", (0.75, 0.90), \"Extreme coastal erosion - disappearing land\"),\n",
    "]\n",
    "\n",
    "# Run validation\n",
    "results = []\n",
    "\n",
    "for idx, (lat, lon, name, dates, expected, expected_range, deg_type) in enumerate(validation_locations, 1):\n",
    "    print(f\"\\n[{idx}/10] Testing: {name}\")\n",
    "    print(f\"   Type: {deg_type}\")\n",
    "    print(f\"   Expected: {expected}\")\n",
    "    \n",
    "    try:\n",
    "        # Fetch image\n",
    "        image_raw, _ = fetch_ee_image(lat, lon, name, dates[0], dates[1])\n",
    "        \n",
    "        if image_raw is None:\n",
    "            print(f\"   ‚ùå Failed to fetch image\")\n",
    "            results.append({\n",
    "                'Location': name,\n",
    "                'Type': deg_type,\n",
    "                'Expected': expected,\n",
    "                'Model Prediction': 'N/A',\n",
    "                'Match': '‚ùå',\n",
    "                'Error': 'N/A',\n",
    "                'Notes': 'Failed to fetch'\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Normalize and run model\n",
    "        image_normalized = normalize_image(image_raw)\n",
    "        image_tensor = torch.from_numpy(image_normalized).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(image_tensor)\n",
    "            prediction = torch.sigmoid(logits).cpu().numpy()[0, 0]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mean_pred = prediction.mean()\n",
    "        median_pred = np.median(prediction)\n",
    "        std_pred = prediction.std()\n",
    "        max_pred = prediction.max()\n",
    "        pct_degraded = (prediction > 0.5).mean()\n",
    "        \n",
    "        # Check if within expected range\n",
    "        in_range = expected_range[0] <= mean_pred <= expected_range[1]\n",
    "        error = abs(mean_pred - (expected_range[0] + expected_range[1]) / 2)\n",
    "        \n",
    "        print(f\"   Model: {mean_pred:.1%} (median: {median_pred:.1%}, std: {std_pred:.2f})\")\n",
    "        print(f\"   Degraded pixels (>50%): {pct_degraded:.1%}\")\n",
    "        print(f\"   {'‚úÖ Within range' if in_range else '‚ö†Ô∏è Outside range'}\")\n",
    "        \n",
    "        # Determine match quality\n",
    "        if error < 0.10:\n",
    "            match = '‚úÖ Excellent'\n",
    "        elif error < 0.20:\n",
    "            match = '‚úì Good'\n",
    "        elif error < 0.30:\n",
    "            match = '~ Fair'\n",
    "        else:\n",
    "            match = '‚ùå Poor'\n",
    "        \n",
    "        # Analyze what might be missed\n",
    "        notes = []\n",
    "        if mean_pred < expected_range[0] - 0.15:\n",
    "            notes.append('Underestimating degradation')\n",
    "        elif mean_pred > expected_range[1] + 0.15:\n",
    "            notes.append('Overestimating degradation')\n",
    "        \n",
    "        if std_pred > 0.25:\n",
    "            notes.append('High spatial variability')\n",
    "        \n",
    "        if pct_degraded > 0.7 and mean_pred < 0.5:\n",
    "            notes.append('Many high-prob pixels but low mean')\n",
    "        \n",
    "        results.append({\n",
    "            'Location': name,\n",
    "            'Type': deg_type.split(' - ')[0],\n",
    "            'Expected': expected,\n",
    "            'Model Mean': f\"{mean_pred:.1%}\",\n",
    "            'Model Range': f\"{prediction.min():.1%}-{max_pred:.1%}\",\n",
    "            'Match': match,\n",
    "            'Error': f\"{error:.1%}\",\n",
    "            'Notes': '; '.join(notes) if notes else 'Good match'\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "        results.append({\n",
    "            'Location': name,\n",
    "            'Type': deg_type,\n",
    "            'Expected': expected,\n",
    "            'Model Prediction': 'ERROR',\n",
    "            'Match': '‚ùå',\n",
    "            'Error': 'N/A',\n",
    "            'Notes': str(e)[:50]\n",
    "        })\n",
    "\n",
    "# Create summary table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä VALIDATION RESULTS SUMMARY\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "display(df)\n",
    "\n",
    "# Calculate statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìà PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "excellent = sum(1 for r in results if '‚úÖ' in r['Match'])\n",
    "good = sum(1 for r in results if '‚úì' in r['Match'])\n",
    "fair = sum(1 for r in results if '~' in r['Match'])\n",
    "poor = sum(1 for r in results if '‚ùå' in r['Match'])\n",
    "\n",
    "print(f\"Match Quality:\")\n",
    "print(f\"   ‚úÖ Excellent (<10% error): {excellent}/10\")\n",
    "print(f\"   ‚úì Good (10-20% error): {good}/10\")\n",
    "print(f\"   ~ Fair (20-30% error): {fair}/10\")\n",
    "print(f\"   ‚ùå Poor (>30% error): {poor}/10\")\n",
    "\n",
    "# Analyze by degradation category\n",
    "print(f\"\\nPerformance by Category:\")\n",
    "for category in ['High', 'Medium', 'Low', 'Very High']:\n",
    "    cat_results = [r for r in results if category.lower() in r['Expected'].lower()]\n",
    "    if cat_results:\n",
    "        cat_excellent = sum(1 for r in cat_results if '‚úÖ' in r['Match'])\n",
    "        print(f\"   {category} Degradation: {cat_excellent}/{len(cat_results)} excellent\")\n",
    "\n",
    "# Common issues\n",
    "print(f\"\\nCommon Issues:\")\n",
    "all_notes = [r['Notes'] for r in results if r['Notes'] != 'Good match']\n",
    "if all_notes:\n",
    "    underestimate = sum(1 for n in all_notes if 'Underestimating' in n)\n",
    "    overestimate = sum(1 for n in all_notes if 'Overestimating' in n)\n",
    "    variability = sum(1 for n in all_notes if 'variability' in n)\n",
    "    \n",
    "    if underestimate > 0:\n",
    "        print(f\"   ‚ö†Ô∏è Underestimating degradation: {underestimate} cases\")\n",
    "    if overestimate > 0:\n",
    "        print(f\"   ‚ö†Ô∏è Overestimating degradation: {overestimate} cases\")\n",
    "    if variability > 0:\n",
    "        print(f\"   ‚ö†Ô∏è High spatial variability: {variability} cases\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ No major issues detected!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ CONCLUSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "accuracy = (excellent + good) / len(results) * 100\n",
    "print(f\"\\nOverall Accuracy: {accuracy:.0f}% (Excellent + Good matches)\")\n",
    "\n",
    "if accuracy >= 80:\n",
    "    print(\"‚úÖ Model performs EXCELLENTLY on diverse ecosystems\")\n",
    "elif accuracy >= 60:\n",
    "    print(\"‚úì Model performs WELL with some room for improvement\")\n",
    "elif accuracy >= 40:\n",
    "    print(\"~ Model performs FAIRLY but needs refinement\")\n",
    "else:\n",
    "    print(\"‚ùå Model needs significant improvement\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
]

# Read notebook
print("Reading misc.ipynb...")
with open(notebook_path, 'r') as f:
    nb = json.load(f)

# Create new cell
new_cell = {
    "cell_type": "code",
    "execution_count": None,
    "id": "final_validation_test",
    "metadata": {},
    "outputs": [],
    "source": validation_cell_code
}

# Add cell at the end
nb['cells'].append(new_cell)

# Add a markdown cell before it
markdown_cell = {
    "cell_type": "markdown",
    "id": "validation_header",
    "metadata": {},
    "source": [
        "---\n",
        "## üî¨ Final Model Validation Test\n",
        "Comprehensive evaluation on 10 diverse locations with expert predictions vs model predictions"
    ]
}

nb['cells'].insert(-1, markdown_cell)

# Save
with open(notebook_path, 'w') as f:
    json.dump(nb, f, indent=1)

print("‚úÖ Added final validation test cell to misc.ipynb")
print("\nThe cell will:")
print("  1. Test 10 diverse locations (fire scars, deforestation, wetlands, healthy)")
print("  2. Compare expert predictions vs model predictions")
print("  3. Show results in a detailed table")
print("  4. Analyze performance by degradation category")
print("  5. Identify common issues (over/underestimation, variability)")
print("\n" + "="*70)
